{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chain import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveChararcterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=”1a235cd3-71a9-46af-9cd6-9867ac1028a1”\n",
    "PINECONE_API_ENV=” ”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACT DATA FROM PDF\n",
    "def load_pdf(data):\n",
    "         loader=DirectoryLoader(data,glob=”*.pdf”,Loader_cls=PyPDFLoader)\n",
    "         documents=loader.load()\n",
    "         return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data= load_pdf(\"/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create text chunks\n",
    "def text_split(extracted_dadta):\n",
    "    text_splitter=RecursiveChararcterTextSplitter(chunk_size = 500,chunk_overlap = 20)\n",
    "    text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks\n",
    "text_chunks = text_split(extracted)\n",
    "print(\"length of my chunk:\",len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To download the model\n",
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name= \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the embedding(if its able to convert text to vectors)\n",
    "query_result = embeddings.embed_query(\"Hello World\")\n",
    "print(\"Length\",len(query_result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying embedding on data\n",
    "pinecone.init(api_key=PINECONE_API_KEY,envirnment=PINE_API_ENV)\n",
    "\n",
    "index_name=\"medical-chatbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Embeddings for Each of the Text Chunks and Sorting\n",
    "docsearch = Pinecone.from_text([t.page_conent for t in text_chunks],embeddings,index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we already have an index we can load it like this\n",
    "docsearch=Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "query  = \"What are allergies\"\n",
    "\n",
    "docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Print\",docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt_template=\"\"\"\n",
    "use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer,just say that you don't know,dont try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=Prompt_template, input_variables=[\"context\",\"question\"])\n",
    "chain_type_kwargs={\"prompt\":PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model=\"model/Llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                    model_type=\"llma\",\n",
    "                    config={'max_new_tokens':512,\n",
    "                            'temperature':0.8}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create question-answer object\n",
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuffs\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k':2}),\n",
    "    return_source_kwargs=chain_type_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to give the optimum answer\n",
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa({\"query\": user_input})\n",
    "    print(\"Response :\",result[[\"result\"]])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
